{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5190b54c-d49c-4d6f-80cc-22555336a9cd",
   "metadata": {},
   "source": [
    "# Week 2 - Preprocessing, part 2\n",
    "\n",
    "# 1. Lesson: None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c4e5ff-b05f-4ef2-96f1-49dcb5beb158",
   "metadata": {},
   "source": [
    "# 2. Weekly graph question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad37e29-6e84-41fa-886d-abc1312213ab",
   "metadata": {},
   "source": [
    "The Storytelling With Data book mentions planning on a \"Who, What, and How\" for your data story.  Write down a possible Who, What, and How for your data, using the ideas in the book."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1b4b5b",
   "metadata": {},
   "source": [
    "*DATASET*\n",
    "Flight delay dataset\n",
    "\n",
    "*WHO* \n",
    "Airline Operations Leaders who want to improve the flight experience for customers and reduce operational costs.\n",
    "\n",
    "*WHAT*\n",
    "They want to know which factors contribute flight delays in order to optimize the deployment of resources and communication to customers.\n",
    "\n",
    "*HOW*\n",
    "The Flight Delay dataset can help idenitfy delay factors by route, weather, temperature, airline, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898eb327-aefd-4ac0-b95a-92b616a2181b",
   "metadata": {},
   "source": [
    "# 3. Homework - work with your own data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe925521-979f-4983-8d85-8db8d1316e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14836788-b235-4cd4-b94d-5f749c6141a8",
   "metadata": {},
   "source": [
    "This week, you will do the same types of exercises as last week, but you should use your chosen datasets that someone in your class found last semester. (They likely will not be the particular datasets that you found yourself.)\n",
    "\n",
    "### Here are some types of analysis you can do  Use Google, documentation, and ChatGPT to help you:\n",
    "\n",
    "- Summarize the datasets using info() and describe()\n",
    "\n",
    "- Are there any duplicate rows?\n",
    "\n",
    "- Are there any duplicate values in a given column (when this would be inappropriate?)\n",
    "\n",
    "- What are the mean, median, and mode of each column?\n",
    "\n",
    "- Are there any missing or null values?\n",
    "\n",
    "    - Do you want to fill in the missing value with a mean value?  A value of your choice?  Remove that row?\n",
    "\n",
    "- Identify any other inconsistent data (e.g. someone seems to be taking an action before they are born.)\n",
    "\n",
    "- Encode any categorical variables (e.g. with one-hot encoding.)\n",
    "\n",
    "### Conclusions:\n",
    "\n",
    "- Are the data usable?  If not, find some new data!\n",
    "\n",
    "- Do you need to modify or correct the data in some way?\n",
    "\n",
    "- Is there any class imbalance?  (Categories that have many more items than other categories)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c4f459",
   "metadata": {},
   "source": [
    "# Flight Delay Dataset Examination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096af675",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "- Overall this is a strong dataset with a lot of detail around flights delays and potential causal columns for weather and temperature.\n",
    "- There are some ID columns that need to be mapped if possible.\n",
    "\n",
    "### Class Imbalance\n",
    "- THere doesn't appear to be a significant amount of class imbalance.\n",
    "- Small hub airport origin/destination is 15%\n",
    "- temp_ninfty_n10 is 0.86%\n",
    "- temp_n10_0 is 5.75%\n",
    "- temp_40_infty is 0.33%\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d17c09b",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "### Info on Dataset\n",
    "- 1.2m rows, 61 columns\n",
    "- No rows are all null\n",
    "- There are almost no null values. The only exception is arrdelay which has ~3k null rows.\n",
    "- There are zero duplicate rows.\n",
    "\n",
    "### Stats on each column\n",
    "- depdelay has a min value of -866\n",
    "- There's a temperature of -42 celsius\n",
    "- The rest of the columns look strong and consistent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b30c8c",
   "metadata": {},
   "source": [
    "## Code Section: Flight Delay Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c975e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"DelayData.csv\", delimiter=\",\", low_memory=False)\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('[^a-zA-Z0-9_]', '', regex=True).str.replace('__','_')\n",
    "df_clean = df.dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9dbe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_info_columns', 200)\n",
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16d4b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3315a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "duplicate_rows = df_clean.duplicated().sum()\n",
    "print(f\"\\nNumber of duplicate rows: {duplicate_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8df799",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyze stats for each column\n",
    "\n",
    "# Show all rows and columns in output\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 20)\n",
    "\n",
    "# Prevent truncation of wide columns\n",
    "pd.set_option('display.width', 150)\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "\n",
    "# Select numeric columns excluding those ending with '_id'\n",
    "numeric_df = df_clean.select_dtypes(include=['number'])\n",
    "numeric_df = numeric_df.loc[:, ~numeric_df.columns.str.endswith('_id')]\n",
    "\n",
    "# Calculate statistics for each numeric column\n",
    "mean_values = numeric_df.mean()\n",
    "median_values = numeric_df.median()\n",
    "mode_values = numeric_df.mode().iloc[0]\n",
    "min_values = numeric_df.min()\n",
    "max_values = numeric_df.max()\n",
    "variance_values = numeric_df.var()\n",
    "std_dev_values = numeric_df.std()\n",
    "\n",
    "# Create a DataFrame to summarize the statistics\n",
    "summary_df = pd.DataFrame({\n",
    "    'Column Name': numeric_df.columns,\n",
    "    'Mean': mean_values,\n",
    "    'Median': median_values,\n",
    "    'Mode': mode_values,\n",
    "    'Min': min_values,\n",
    "    'Max': max_values,\n",
    "    'Variance': variance_values,\n",
    "    'Standard Deviation': std_dev_values\n",
    "})\n",
    "\n",
    "# Set 'Column Name' as the index\n",
    "summary_df.set_index('Column Name', inplace=True)\n",
    "\n",
    "# Display the summary DataFrame\n",
    "print(summary_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90216a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one-hot encoding on all object-type (categorical) columns\n",
    "encode_cols = ['origin','dest', 'uniquecarrier', 'originstate', 'origincityname']\n",
    "df_encoded = pd.get_dummies(df_clean, columns=encode_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0091e762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to identify binary columns and measure class imbalance in percentages\n",
    "def measure_binary_class_imbalance(df):\n",
    "    binary_columns = [col for col in df.columns if df[col].nunique() == 2]\n",
    "    imbalance_info = {}\n",
    "\n",
    "    for column in binary_columns:\n",
    "        value_counts = df[column].value_counts()\n",
    "        total = value_counts.sum()\n",
    "        percentage_distribution = (value_counts / total) * 100\n",
    "        imbalance_info[column] = percentage_distribution\n",
    "\n",
    "    return imbalance_info\n",
    "\n",
    "# Measure class imbalance in binary columns\n",
    "binary_imbalance_info = measure_binary_class_imbalance(df_clean)\n",
    "\n",
    "# Print imbalance information\n",
    "for column, value_counts in binary_imbalance_info.items():\n",
    "    print(f\"Column: {column}\")\n",
    "    print(value_counts.round(2))  # Rounded for readability\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d51d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to identify class imbalance in categorical columns\n",
    "def identify_class_imbalance(df):\n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "    imbalance_info = {}\n",
    "\n",
    "    for column in categorical_columns:\n",
    "        if df[column].nunique() <= 100:  # Exclude columns with too many unique values\n",
    "            value_counts = df[column].value_counts()\n",
    "            imbalance_info[column] = value_counts\n",
    "\n",
    "    return imbalance_info\n",
    "\n",
    "# Identify class imbalance\n",
    "imbalance_info = identify_class_imbalance(df_clean)\n",
    "\n",
    "# Visualize class imbalance\n",
    "for column, value_counts in imbalance_info.items():\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    value_counts.plot(kind='bar')\n",
    "    plt.title(f'Class Imbalance in {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639c9119",
   "metadata": {},
   "source": [
    "# USDOT On Time Flight Reporting Dataset Examination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7e6bc5",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "- Overall this looks like a strong dataset with lots of opportunity to analyze and train. It would be ideal to download additional months of data for completeness.\n",
    "\n",
    "### Class Imbalance\n",
    "- There are a high proportion of flights through California, Florida, and Texas. This makes sense given the population and tourism in these states.\n",
    "- Only 1.5% of flights were cancelled.\n",
    "- Only 0.2% of flights were diverted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f13ccd",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "### INFO() on the base dataset\n",
    "- 109 columns, 504,884 rows\n",
    "- All the DIV4 and DIV5 columns are null\n",
    "- Approx 7k rows are missing key data around departure delays.\n",
    "\n",
    "### Analyzing DESCRIBE()\n",
    "- There are lots of ID columns that require lookup files\n",
    "- This dataset only includes data for February 2025. It would be useful to get additional months of data.\n",
    "\n",
    "#### Mean, Median, Model values:\n",
    "- The output is quite large and is stored in summary_df\n",
    "                      \n",
    "#### Missing values:\n",
    "- There's missing values on first, second, and third diversion columns. This makes sense if a flight wasn't diverted\n",
    "\n",
    "#### Inconsistent Data\n",
    "- There are no duplicate rows\n",
    "- There are rows with negative values for delay time. I'm interpretting this as flights leaving or arriving early.\n",
    "- There are 3 columns which capture origin state and another 3 that capture destination state. These can be collapse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163ec3bb",
   "metadata": {},
   "source": [
    "## Code Section: USDOT On Time Flight Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ffe001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the On Time Flight Dataset\n",
    "df = pd.read_csv(\"T_ONTIME_REPORTING.csv\", delimiter=\",\", low_memory=False)\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('[^a-zA-Z0-9_]', '', regex=True).str.replace('__','_')\n",
    "df_clean = df.dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1008b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_info_columns', 200)\n",
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a135479",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e1f746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "duplicate_rows = df_clean.duplicated().sum()\n",
    "print(f\"\\nNumber of duplicate rows: {duplicate_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ea753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyze stats for each column\n",
    "\n",
    "# Show all rows and columns in output\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 20)\n",
    "\n",
    "# Prevent truncation of wide columns\n",
    "pd.set_option('display.width', 150)\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "\n",
    "# Select numeric columns excluding those ending with '_id'\n",
    "numeric_df = df_clean.select_dtypes(include=['number'])\n",
    "numeric_df = numeric_df.loc[:, ~numeric_df.columns.str.endswith('_id')]\n",
    "\n",
    "# Calculate statistics for each numeric column\n",
    "mean_values = numeric_df.mean()\n",
    "median_values = numeric_df.median()\n",
    "mode_values = numeric_df.mode().iloc[0]\n",
    "min_values = numeric_df.min()\n",
    "max_values = numeric_df.max()\n",
    "variance_values = numeric_df.var()\n",
    "std_dev_values = numeric_df.std()\n",
    "\n",
    "# Create a DataFrame to summarize the statistics\n",
    "summary_df = pd.DataFrame({\n",
    "    'Column Name': numeric_df.columns,\n",
    "    'Mean': mean_values,\n",
    "    'Median': median_values,\n",
    "    'Mode': mode_values,\n",
    "    'Min': min_values,\n",
    "    'Max': max_values,\n",
    "    'Variance': variance_values,\n",
    "    'Standard Deviation': std_dev_values\n",
    "})\n",
    "\n",
    "# Set 'Column Name' as the index\n",
    "summary_df.set_index('Column Name', inplace=True)\n",
    "\n",
    "# Display the summary DataFrame\n",
    "print(summary_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9737bceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one-hot encoding on all object-type (categorical) columns\n",
    "encode_cols = ['op_unique_carrier','op_carrier','origin','origin_city_name','origin_state_abr','dest','dest_city_name','dest_state_abr','dep_time_blk','arr_time_blk','cancellation_code']\n",
    "df_encoded = pd.get_dummies(df_clean, columns=encode_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e874ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to identify class imbalance in categorical columns\n",
    "def identify_class_imbalance(df):\n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "    imbalance_info = {}\n",
    "\n",
    "    for column in categorical_columns:\n",
    "        if df[column].nunique() <= 100:  # Exclude columns with too many unique values\n",
    "            value_counts = df[column].value_counts()\n",
    "            imbalance_info[column] = value_counts\n",
    "\n",
    "    return imbalance_info\n",
    "\n",
    "# Identify class imbalance\n",
    "imbalance_info = identify_class_imbalance(df_clean)\n",
    "\n",
    "# Visualize class imbalance\n",
    "for column, value_counts in imbalance_info.items():\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    value_counts.plot(kind='bar')\n",
    "    plt.title(f'Class Imbalance in {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3414074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to identify binary columns and measure class imbalance in percentages\n",
    "def measure_binary_class_imbalance(df):\n",
    "    binary_columns = [col for col in df.columns if df[col].nunique() == 2]\n",
    "    imbalance_info = {}\n",
    "\n",
    "    for column in binary_columns:\n",
    "        value_counts = df[column].value_counts()\n",
    "        total = value_counts.sum()\n",
    "        percentage_distribution = (value_counts / total) * 100\n",
    "        imbalance_info[column] = percentage_distribution\n",
    "\n",
    "    return imbalance_info\n",
    "\n",
    "# Measure class imbalance in binary columns\n",
    "binary_imbalance_info = measure_binary_class_imbalance(df_clean)\n",
    "\n",
    "# Print imbalance information\n",
    "for column, value_counts in binary_imbalance_info.items():\n",
    "    print(f\"Column: {column}\")\n",
    "    print(value_counts.round(2))  # Rounded for readability\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f58845d",
   "metadata": {},
   "source": [
    "# Priceline Flight Dataset Examination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67501a0a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "- After several data cleansing steps the datasets appears functional. However, the data lack significant utility as it only captures pricing data for 2 days in April 2025 and lacks significant breadth or depth.\n",
    "- None of the columns show signifcant class imbalance. Though 25% of the records list \"Multiple\" airlines.\n",
    "- Additional features that may be interesting: day of week, day of month, month of year, hour of day, region/continent, departure country, arrival country, domestic/international flight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedad6e5",
   "metadata": {},
   "source": [
    "## Observations\n",
    "### Data Quality\n",
    "- There are several empty columns. The CSV appears to be corrupt. We will remove the bad columns.\n",
    "- Date and Time columns are text.\n",
    "- Price column is an object, not numeric.\n",
    "- There are some rows of bad data. Dollar sign in the price column. Number of Stops = Express Deal. Travel Time = Save $39. etc. These rows will be removed.\n",
    "### INFO() on the base dataset\n",
    "- 15 columns, 2,459 rows\n",
    "- 2 rows missing an airline name\n",
    "- 296 rows missing a departure airport\n",
    "- 3 rows missing a ticket price\n",
    "- 299 rows missing an arrival airport (maybe they never left?)\n",
    "- Several rows don't have 2nd or 3rd stoppage values. These are most likely flights that were one leg\n",
    "### Analyzing the clean dataset\n",
    "- 57 unique airlines\n",
    "- Travel time rangs from ~2.66 hours to 82 hours\n",
    "- Wait times range from 39 minutes to 390 minutes\n",
    "- Ticker prices ranges from $135 to $8000\n",
    "- 28 duplicate rows\n",
    "\n",
    "#### Mean values:\n",
    "- travel_time_minutes      1551.063440\n",
    "- 1st_stop_wait_minutes     510.922116\n",
    "- 2nd_stop_wait_minutes     426.108563\n",
    "- 3rd_stop_wait_minutes     356.764706\n",
    "- stops                       1.294429\n",
    "- ticket_price_usd         1316.544603\n",
    "\n",
    "#### Median values:\n",
    "- travel_time_minutes      1430.0\n",
    "- 1st_stop_wait_minutes     424.0\n",
    "- 2nd_stop_wait_minutes     260.0\n",
    "- 3rd_stop_wait_minutes     200.0\n",
    "- stops                       1.0\n",
    "- ticket_price_usd         1128.0\n",
    "\n",
    "#### Mode values:\n",
    "- travel_time_minutes       965.0\n",
    "- 1st_stop_wait_minutes     130.0\n",
    "- 2nd_stop_wait_minutes     240.0\n",
    "- 3rd_stop_wait_minutes     200.0\n",
    "- stops                       1.0\n",
    "- ticket_price_usd         1508.0\n",
    "\n",
    "#### Missing values:\n",
    "- airline_name                2 (These should dropped)\n",
    "- depreture_airport         296 (These need to be investigated and potentially dropped)\n",
    "- 1st_stoppage               59 (It is valid that a flight may not have a stopover)\n",
    "- 2nd_stoppage             1803 (It is valid that a flight may not have a stopover)\n",
    "- 3rd_stoppage             2442 (It is valid that a flight may not have a stopover)\n",
    "- destination_airport       293 (These need to be investigated and potentially dropped)\n",
    "- travel_time_minutes         0\n",
    "- 1st_stop_wait_minutes      58 (It is valid that a flight may not have a stopover) \n",
    "- 2nd_stop_wait_minutes    1805 (It is valid that a flight may not have a stopover)\n",
    "- 3rd_stop_wait_minutes    2442 (It is valid that a flight may not have a stopover)\n",
    "- stops                       0\n",
    "- ticket_price_usd            4 (Impute using mean or regression)\n",
    "- arrival_datetime          164 (These need to be investigated and potentially dropped)\n",
    "- departure_time_24hr         0\n",
    "\n",
    "#### Inconsistent Data\n",
    "- It doesn't seem reasonable that a flight would be missing a departure or arrival airport.\n",
    "- All wait times, stops, and prices seem valid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd206ed",
   "metadata": {},
   "source": [
    "## Code Section: Priceline Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4517f6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Flight Dataset\n",
    "df_src = pd.read_csv(\"flight.csv\", delimiter=\",\", low_memory=False)\n",
    "\n",
    "def flight_data_cleaner(df):\n",
    "    # Remove the junk columns\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "    # Clean up the column names\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('[^a-zA-Z0-9_]', '', regex=True).str.replace('__','_')\n",
    "    return df\n",
    "\n",
    "def convert_to_minutes(time_str):\n",
    "    if pd.isna(time_str):\n",
    "        return np.nan\n",
    "    if isinstance(time_str, float):\n",
    "        return np.nan\n",
    "    try:\n",
    "        total_minutes = 0\n",
    "        if 'h' in time_str:\n",
    "            hours, minutes = time_str.split('h ')\n",
    "            total_minutes += int(hours) * 60\n",
    "            total_minutes += int(minutes.replace('m', ''))\n",
    "        else:\n",
    "            total_minutes += int(time_str.replace('m', ''))\n",
    "        return total_minutes\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "df_src = flight_data_cleaner(df_src)\n",
    "df_clean = df_src.copy()\n",
    "df_clean['travel_time_minutes'] = df_clean['travel_time'].apply(convert_to_minutes)\n",
    "df_clean['1st_stop_wait_minutes'] = df_clean['1st_stoppage_waiting_hour'].apply(convert_to_minutes)\n",
    "df_clean['2nd_stop_wait_minutes'] = df_clean['2nd_stoppagewaiting_time'].apply(convert_to_minutes)\n",
    "df_clean['3rd_stop_wait_minutes'] = df_clean['3rd_stoppage_waiting_time'].apply(convert_to_minutes)\n",
    "\n",
    "\n",
    "# Map the stops column\n",
    "stops_mapping = {\n",
    "    'Nonstop': 0,\n",
    "    '1 Stop': 1,\n",
    "    '2 Stops': 2,\n",
    "    '3 Stops': 3\n",
    "}\n",
    "df_clean['stops'] = df_clean['number_of_stoppage'].map(stops_mapping)\n",
    "\n",
    "# Replace any value containing a dollar sign with None\n",
    "df_clean['ticket_prizedoller'] = df_clean['ticket_prizedoller'].apply(lambda x: None if '$' in str(x) or 'Alaska' in str(x) else x)\n",
    "# Convert the entire column to float\n",
    "df_clean['ticket_price_usd'] = df_clean['ticket_prizedoller'].astype(float)\n",
    "\n",
    "\n",
    "def clean_and_combine_datetime(df, date_col, time_col):\n",
    "    def parse_datetime(date_str, time_str):\n",
    "        try:\n",
    "            # Strip leading/trailing spaces and prefixes\n",
    "            if isinstance(date_str, str):\n",
    "                date_str = date_str.strip()\n",
    "                if 'Arrives:' in date_str:\n",
    "                    date_str = date_str.split('Arrives: ')[-1]\n",
    "                date_str = date_str.split(', ')[-1]\n",
    "            else:\n",
    "                return None\n",
    "            \n",
    "            if isinstance(time_str, str):\n",
    "                time_str = time_str.strip().upper().replace('A', 'AM').replace('P', 'PM')\n",
    "            else:\n",
    "                return None\n",
    "            \n",
    "            # Add current year to date string\n",
    "            current_year = datetime.now().year\n",
    "            date_str = f\"{date_str} {current_year}\"\n",
    "            \n",
    "            # Combine date and time strings\n",
    "            datetime_str = f\"{date_str} {time_str}\"\n",
    "            \n",
    "            # Parse combined datetime string\n",
    "            return datetime.strptime(datetime_str, '%b %d %Y %I:%M%p')\n",
    "        except ValueError:\n",
    "            return None\n",
    "    \n",
    "    # Apply the parsing function to the DataFrame\n",
    "    df['arrival_datetime'] = df.apply(lambda row: parse_datetime(row[date_col], row[time_col]), axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Clean and combine datetime\n",
    "df_clean = clean_and_combine_datetime(df, 'arrival_date', 'arrival_time')\n",
    "\n",
    "def convert_to_24_hour(time_str):\n",
    "    try:\n",
    "        # Check if the value is a string\n",
    "        if isinstance(time_str, str):\n",
    "            # Normalize lowercase 'a'/'p' to 'AM'/'PM'\n",
    "            time_str = time_str.replace('a', 'AM').replace('p', 'PM')\n",
    "            # Convert to 24-hour format\n",
    "            return datetime.strptime(time_str, '%I:%M%p').strftime('%H:%M')\n",
    "        else:\n",
    "            return None\n",
    "    except (ValueError, TypeError):\n",
    "        # Return None for invalid or missing values\n",
    "        return None\n",
    "\n",
    "# Apply the conversion function to the 'depreture_time' column\n",
    "df_clean['departure_time_24hr'] = df_clean['depreture_time'].apply(convert_to_24_hour)\n",
    "df_clean.rename(columns={'depreture_airport': 'departure_airport'}, inplace=True)\n",
    "\n",
    "\n",
    "#Remove all the junk columns\n",
    "junk_cols = {'travel_time', 'number_of_stoppage','depreture_time','ticket_prizedoller','1st_stoppage_waiting_hour','2nd_stoppagewaiting_time','3rd_stoppage_waiting_time','arrival_date','arrival_time'}\n",
    "\n",
    "df_clean = df_clean.drop(columns=junk_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06605ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_src.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e1fb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.describe()\n",
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8d4d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "duplicate_rows = df_clean.duplicated().sum()\n",
    "print(f\"\\nNumber of duplicate rows: {duplicate_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b550c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter numeric columns\n",
    "numeric_df = df_clean.select_dtypes(include=['number'])\n",
    "\n",
    "# Calculate mean, median, and mode of each numeric column\n",
    "mean_values = numeric_df.mean()\n",
    "median_values = numeric_df.median()\n",
    "mode_values = numeric_df.mode().iloc[0]\n",
    "\n",
    "print(\"\\nMean values:\")\n",
    "print(mean_values)\n",
    "print(\"\\nMedian values:\")\n",
    "print(median_values)\n",
    "print(\"\\nMode values:\")\n",
    "print(mode_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dca1b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing or null values\n",
    "missing_values = df_clean.isnull().sum()\n",
    "print(\"\\nMissing values:\")\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3560c3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with mean\n",
    "df['ticket_price_usd'] = df['ticket_price_usd'].fillna(df_clean['ticket_price_usd'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462b47e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one-hot encoding on all object-type (categorical) columns\n",
    "encode_cols = ['airline_name','departure_airport', '1st_stoppage','2nd_stoppage','3rd_stoppage','destination_airport']\n",
    "df_encoded = pd.get_dummies(df_clean, columns=encode_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b477ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to identify class imbalance in categorical columns\n",
    "def identify_class_imbalance(df):\n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "    imbalance_info = {}\n",
    "    \n",
    "    for column in categorical_columns:\n",
    "        value_counts = df[column].value_counts()\n",
    "        imbalance_info[column] = value_counts\n",
    "    \n",
    "    return imbalance_info\n",
    "\n",
    "# Identify class imbalance\n",
    "imbalance_info = identify_class_imbalance(df_clean)\n",
    "\n",
    "# Print imbalance information\n",
    "for column, value_counts in imbalance_info.items():\n",
    "    print(f\"Column: {column}\")\n",
    "    print(value_counts)\n",
    "    print()\n",
    "\n",
    "# Visualize class imbalance\n",
    "for column, value_counts in imbalance_info.items():\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    value_counts.plot(kind='bar')\n",
    "    plt.title(f'Class Imbalance in {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abab9e6d-18cc-4863-b980-3e52f581763a",
   "metadata": {},
   "source": [
    "# 4. Storytelling With Data graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1911148d-9df6-4b33-a875-8c96408ec834",
   "metadata": {},
   "source": [
    "Just like last week: choose any graph in the Introduction of Storytelling With Data. Use matplotlib to reproduce it in a rough way. I don't expect you to spend an enormous amount of time on this; I understand that you likely will not have time to re-create every feature of the graph. However, if you're excited about learning to use matplotlib, this is a good way to do that. You don't have to duplicate the exact values on the graph; just the same rough shape will be enough.  If you don't feel comfortable using matplotlib yet, do the best you can and write down what you tried or what Google searches you did to find the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2888f9-3700-45ab-9829-6a5372106f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
