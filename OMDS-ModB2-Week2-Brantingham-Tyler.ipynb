{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5190b54c-d49c-4d6f-80cc-22555336a9cd",
   "metadata": {},
   "source": [
    "# Week 2 - Preprocessing, part 2\n",
    "\n",
    "# 1. Lesson: None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c4e5ff-b05f-4ef2-96f1-49dcb5beb158",
   "metadata": {},
   "source": [
    "# 2. Weekly graph question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad37e29-6e84-41fa-886d-abc1312213ab",
   "metadata": {},
   "source": [
    "The Storytelling With Data book mentions planning on a \"Who, What, and How\" for your data story.  Write down a possible Who, What, and How for your data, using the ideas in the book."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1b4b5b",
   "metadata": {},
   "source": [
    "*DATASET*\n",
    "Flight delay dataset\n",
    "\n",
    "*WHO* \n",
    "Airline Operations Leaders who want to improve the flight experience for customers and reduce operational costs.\n",
    "\n",
    "*WHAT*\n",
    "They want to know which factors contribute flight delays in order to optimize the deployment of resources and communication to customers.\n",
    "\n",
    "*HOW*\n",
    "The Flight Delay dataset can help idenitfy delay factors by route, weather, temperature, airline, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898eb327-aefd-4ac0-b95a-92b616a2181b",
   "metadata": {},
   "source": [
    "# 3. Homework - work with your own data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe925521-979f-4983-8d85-8db8d1316e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14836788-b235-4cd4-b94d-5f749c6141a8",
   "metadata": {},
   "source": [
    "This week, you will do the same types of exercises as last week, but you should use your chosen datasets that someone in your class found last semester. (They likely will not be the particular datasets that you found yourself.)\n",
    "\n",
    "### Here are some types of analysis you can do  Use Google, documentation, and ChatGPT to help you:\n",
    "\n",
    "- Summarize the datasets using info() and describe()\n",
    "\n",
    "- Are there any duplicate rows?\n",
    "\n",
    "- Are there any duplicate values in a given column (when this would be inappropriate?)\n",
    "\n",
    "- What are the mean, median, and mode of each column?\n",
    "\n",
    "- Are there any missing or null values?\n",
    "\n",
    "    - Do you want to fill in the missing value with a mean value?  A value of your choice?  Remove that row?\n",
    "\n",
    "- Identify any other inconsistent data (e.g. someone seems to be taking an action before they are born.)\n",
    "\n",
    "- Encode any categorical variables (e.g. with one-hot encoding.)\n",
    "\n",
    "### Conclusions:\n",
    "\n",
    "- Are the data usable?  If not, find some new data!\n",
    "\n",
    "- Do you need to modify or correct the data in some way?\n",
    "\n",
    "- Is there any class imbalance?  (Categories that have many more items than other categories)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c4f459",
   "metadata": {},
   "source": [
    "# Flight Delay Dataset Examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c975e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"DelayData.csv\", delimiter=\",\")\n",
    "\n",
    "# Summarize the dataset using info() and describe()\n",
    "print(\"DataFrame Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nDataFrame Description:\")\n",
    "print(df.describe(include='all'))\n",
    "\n",
    "# Check for duplicate rows\n",
    "duplicate_rows = df.duplicated().sum()\n",
    "print(f\"\\nNumber of duplicate rows: {duplicate_rows}\")\n",
    "\n",
    "# Check for duplicate values in a given column (example: 'tailnum')\n",
    "duplicate_values_tailnum = df['tailnum'].duplicated().sum()\n",
    "print(f\"\\nNumber of duplicate values in 'tailnum': {duplicate_values_tailnum}\")\n",
    "\n",
    "# Calculate mean, median, and mode of each column\n",
    "# mean_values = df.mean()\n",
    "# median_values = df.median()\n",
    "# mode_values = df.mode().iloc[0]\n",
    "\n",
    "# print(\"\\nMean values:\")\n",
    "# print(mean_values)\n",
    "# print(\"\\nMedian values:\")\n",
    "# print(median_values)\n",
    "# print(\"\\nMode values:\")\n",
    "# print(mode_values)\n",
    "\n",
    "# Check for missing or null values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"\\nMissing values:\")\n",
    "print(missing_values)\n",
    "\n",
    "# # Fill missing values with mean (example: 'temperature')\n",
    "# df['temperature'].fillna(df['temperature'].mean(), inplace=True)\n",
    "# df['windspeed'].fillna(df['windspeed'].mean(), inplace=True)\n",
    "# df['windspeedsquare'].fillna(df['windspeedsquare'].mean(), inplace=True)\n",
    "# df['windgustspeed'].fillna(df['windgustspeed'].mean(), inplace=True)\n",
    "\n",
    "# Identify inconsistent data (example: 'year' should be reasonable)\n",
    "inconsistent_data = df[df['year'] < 1900]\n",
    "print(\"\\nInconsistent data (year < 1900):\")\n",
    "print(inconsistent_data)\n",
    "\n",
    "# Encode categorical variables (example: 'origin', 'dest', 'uniquecarrier')\n",
    "df_encoded = pd.get_dummies(df, columns=['origin', 'dest', 'uniquecarrier', 'origincityname', 'originstate'])\n",
    "\n",
    "print(\"\\nDataFrame with encoded categorical variables:\")\n",
    "print(df_encoded.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb36ec63",
   "metadata": {},
   "source": [
    "# Priceline Flight Dataset Examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e73bda9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tyler.brantingham\\AppData\\Local\\Temp\\ipykernel_31476\\2371142875.py:1: DtypeWarning: Columns (4,11,12,36,37,38,39,40,42,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,64,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,86,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,108,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,130,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,152,154,155,156,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,174,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,218,220,221,222,224,225,227,228,229,230,231,232,233,234,235,236,237,238,240,242,243,244,246,247,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,284,286,287,288,289,290,291,293,294,295,296,297,298,299,300,301,302,303,304,306,308,309,310,311,312,313,315,316,317,318,319,320,321,322,323,324,325,326,328,330,331,332,334,335,337,338,339,340,341,342,343,344,345,346,347,348,350,352,353,354,356,357,359,360,361,362,363,364,365,366,367,368,369,370,372,374,375,376,378,379,381,382,383,384,385,386,387,388,389,390,391,392,394,396,397,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,416,418,419,420,422,423,425,426,427,428,429,430,431,432,433,434,435,436,438,440,441,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,460,462,463,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,482,484,485,486,487,488,489,491,492,493,494) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"flight.csv\", delimiter=\",\")\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"flight.csv\", delimiter=\",\")\n",
    "#df = df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639c9119",
   "metadata": {},
   "source": [
    "# USDOT On Time Flight Reporting Dataset Examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ffe001",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"T_ONTIME_REPORTING.csv\", delimiter=\",\")\n",
    "df = df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedad6e5",
   "metadata": {},
   "source": [
    "# Priceline Flight Dataset Examination\n",
    "\n",
    "## Observations\n",
    "### Data Quality\n",
    "- There are several empty columns. The CSV appears to be corrupt. We will remove the bad columns.\n",
    "- Date and Time columns are text.\n",
    "- Price column is an object, not numeric.\n",
    "- There are some rows of bad data. Dollar sign in the price column. Number of Stops = Express Deal. Travel Time = Save $39. etc. These rows will be removed.\n",
    "### INFO() on the base dataset\n",
    "- 15 columns, 2,459 rows\n",
    "- 2 rows missing an airline name\n",
    "- 296 rows missing a departure airport\n",
    "- 3 rows missing a ticket price\n",
    "- 299 rows missing an arrival airport (maybe they never left?)\n",
    "- Several rows don't have 2nd or 3rd stoppage values. These are most likely flights that were one leg\n",
    "### DESCRIBE() on the clean dataset\n",
    "- 57 unique airlines\n",
    "- Travel time rangs from ~2.66 hours to 82 hours\n",
    "- Wait times range from 39 minutes to 390 minutes\n",
    "- Ticker prices ranges from $135 to $8000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4517f6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Flight Dataset\n",
    "df_src = pd.read_csv(\"flight.csv\", delimiter=\",\", low_memory=False)\n",
    "\n",
    "def flight_data_cleaner(df):\n",
    "    # Remove the junk columns\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "    # Clean up the column names\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('[^a-zA-Z0-9_]', '', regex=True).str.replace('__','_')\n",
    "    return df\n",
    "\n",
    "def convert_to_minutes(time_str):\n",
    "    if pd.isna(time_str):\n",
    "        return np.nan\n",
    "    if isinstance(time_str, float):\n",
    "        return np.nan\n",
    "    try:\n",
    "        total_minutes = 0\n",
    "        if 'h' in time_str:\n",
    "            hours, minutes = time_str.split('h ')\n",
    "            total_minutes += int(hours) * 60\n",
    "            total_minutes += int(minutes.replace('m', ''))\n",
    "        else:\n",
    "            total_minutes += int(time_str.replace('m', ''))\n",
    "        return total_minutes\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "df_src = flight_data_cleaner(df_src)\n",
    "df_clean = df_src.copy()\n",
    "df_clean['travel_time_minutes'] = df_clean['travel_time'].apply(convert_to_minutes)\n",
    "df_clean['1st_stop_wait_minutes'] = df_clean['1st_stoppage_waiting_hour'].apply(convert_to_minutes)\n",
    "df_clean['2nd_stop_wait_minutes'] = df_clean['2nd_stoppagewaiting_time'].apply(convert_to_minutes)\n",
    "df_clean['3rd_stop_wait_minutes'] = df_clean['3rd_stoppage_waiting_time'].apply(convert_to_minutes)\n",
    "\n",
    "\n",
    "# Map the stops column\n",
    "stops_mapping = {\n",
    "    'Nonstop': 0,\n",
    "    '1 Stop': 1,\n",
    "    '2 Stops': 2,\n",
    "    '3 Stops': 3\n",
    "}\n",
    "df_clean['stops'] = df_clean['number_of_stoppage'].map(stops_mapping)\n",
    "\n",
    "# Replace any value containing a dollar sign with None\n",
    "df_clean['ticket_prizedoller'] = df_clean['ticket_prizedoller'].apply(lambda x: None if '$' in str(x) or 'Alaska' in str(x) else x)\n",
    "# Convert the entire column to float\n",
    "df_clean['ticket_price_usd'] = df_clean['ticket_prizedoller'].astype(float)\n",
    "\n",
    "\n",
    "def clean_and_combine_datetime(df, date_col, time_col):\n",
    "    def parse_datetime(date_str, time_str):\n",
    "        try:\n",
    "            # Strip leading/trailing spaces and prefixes\n",
    "            if isinstance(date_str, str):\n",
    "                date_str = date_str.strip()\n",
    "                if 'Arrives:' in date_str:\n",
    "                    date_str = date_str.split('Arrives: ')[-1]\n",
    "                date_str = date_str.split(', ')[-1]\n",
    "            else:\n",
    "                return None\n",
    "            \n",
    "            if isinstance(time_str, str):\n",
    "                time_str = time_str.strip().upper().replace('A', 'AM').replace('P', 'PM')\n",
    "            else:\n",
    "                return None\n",
    "            \n",
    "            # Add current year to date string\n",
    "            current_year = datetime.now().year\n",
    "            date_str = f\"{date_str} {current_year}\"\n",
    "            \n",
    "            # Combine date and time strings\n",
    "            datetime_str = f\"{date_str} {time_str}\"\n",
    "            \n",
    "            # Parse combined datetime string\n",
    "            return datetime.strptime(datetime_str, '%b %d %Y %I:%M%p')\n",
    "        except ValueError:\n",
    "            return None\n",
    "    \n",
    "    # Apply the parsing function to the DataFrame\n",
    "    df['arrival_datetime'] = df.apply(lambda row: parse_datetime(row[date_col], row[time_col]), axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Clean and combine datetime\n",
    "df_clean = clean_and_combine_datetime(df, 'arrival_date', 'arrival_time')\n",
    "\n",
    "\n",
    "def convert_to_24_hour(time_str):\n",
    "    try:\n",
    "        # Check if the value is a string\n",
    "        if isinstance(time_str, str):\n",
    "            # Normalize lowercase 'a'/'p' to 'AM'/'PM'\n",
    "            time_str = time_str.replace('a', 'AM').replace('p', 'PM')\n",
    "            # Convert to 24-hour format\n",
    "            return datetime.strptime(time_str, '%I:%M%p').strftime('%H:%M')\n",
    "        else:\n",
    "            return None\n",
    "    except (ValueError, TypeError):\n",
    "        # Return None for invalid or missing values\n",
    "        return None\n",
    "\n",
    "# Apply the conversion function to the 'depreture_time' column\n",
    "df_clean['departure_time_24hr'] = df_clean['depreture_time'].apply(convert_to_24_hour)\n",
    "\n",
    "\n",
    "#Remove all the junk columns\n",
    "junk_cols = {'travel_time', 'number_of_stoppage','depreture_time','ticket_prizedoller','1st_stoppage_waiting_hour','2nd_stoppagewaiting_time','3rd_stoppage_waiting_time','arrival_date','arrival_time'}\n",
    "\n",
    "df_clean = df_clean.drop(columns=junk_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06605ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_src.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "85e1fb1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "travel_time_minutes",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "1st_stop_wait_minutes",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "2nd_stop_wait_minutes",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "3rd_stop_wait_minutes",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "stops",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ticket_price_usd",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "arrival_datetime",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "c350f321-f38e-407e-9843-fb4b73a0e0f9",
       "rows": [
        [
         "count",
         "2459.0",
         "2401.0",
         "654.0",
         "17.0",
         "2459.0",
         "2455.0",
         "2295"
        ],
        [
         "mean",
         "1551.063440422936",
         "510.92211578508955",
         "426.1085626911315",
         "356.7647058823529",
         "1.294428629524197",
         "1316.5446028513238",
         "2025-04-08 19:46:51.320261376"
        ],
        [
         "min",
         "160.0",
         "39.0",
         "50.0",
         "176.0",
         "0.0",
         "135.0",
         "2025-04-08 00:20:00"
        ],
        [
         "25%",
         "985.0",
         "180.0",
         "170.0",
         "176.0",
         "1.0",
         "771.0",
         "2025-04-08 11:30:00"
        ],
        [
         "50%",
         "1430.0",
         "424.0",
         "260.0",
         "200.0",
         "1.0",
         "1128.0",
         "2025-04-08 18:03:00"
        ],
        [
         "75%",
         "2000.0",
         "770.0",
         "543.5",
         "346.0",
         "2.0",
         "1607.0",
         "2025-04-09 00:05:00"
        ],
        [
         "max",
         "4930.0",
         "1440.0",
         "1435.0",
         "1385.0",
         "3.0",
         "7867.0",
         "2025-04-10 11:00:00"
        ],
        [
         "std",
         "728.4019413499985",
         "376.46050503099025",
         "351.52224925073875",
         "391.95256878412033",
         "0.5177223983794402",
         "885.8856574473365",
         null
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>travel_time_minutes</th>\n",
       "      <th>1st_stop_wait_minutes</th>\n",
       "      <th>2nd_stop_wait_minutes</th>\n",
       "      <th>3rd_stop_wait_minutes</th>\n",
       "      <th>stops</th>\n",
       "      <th>ticket_price_usd</th>\n",
       "      <th>arrival_datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2459.000000</td>\n",
       "      <td>2401.000000</td>\n",
       "      <td>654.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>2459.000000</td>\n",
       "      <td>2455.000000</td>\n",
       "      <td>2295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1551.063440</td>\n",
       "      <td>510.922116</td>\n",
       "      <td>426.108563</td>\n",
       "      <td>356.764706</td>\n",
       "      <td>1.294429</td>\n",
       "      <td>1316.544603</td>\n",
       "      <td>2025-04-08 19:46:51.320261376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>160.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>2025-04-08 00:20:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>985.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>771.000000</td>\n",
       "      <td>2025-04-08 11:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1430.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>260.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1128.000000</td>\n",
       "      <td>2025-04-08 18:03:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2000.000000</td>\n",
       "      <td>770.000000</td>\n",
       "      <td>543.500000</td>\n",
       "      <td>346.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1607.000000</td>\n",
       "      <td>2025-04-09 00:05:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4930.000000</td>\n",
       "      <td>1440.000000</td>\n",
       "      <td>1435.000000</td>\n",
       "      <td>1385.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7867.000000</td>\n",
       "      <td>2025-04-10 11:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>728.401941</td>\n",
       "      <td>376.460505</td>\n",
       "      <td>351.522249</td>\n",
       "      <td>391.952569</td>\n",
       "      <td>0.517722</td>\n",
       "      <td>885.885657</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       travel_time_minutes  1st_stop_wait_minutes  2nd_stop_wait_minutes  \\\n",
       "count          2459.000000            2401.000000             654.000000   \n",
       "mean           1551.063440             510.922116             426.108563   \n",
       "min             160.000000              39.000000              50.000000   \n",
       "25%             985.000000             180.000000             170.000000   \n",
       "50%            1430.000000             424.000000             260.000000   \n",
       "75%            2000.000000             770.000000             543.500000   \n",
       "max            4930.000000            1440.000000            1435.000000   \n",
       "std             728.401941             376.460505             351.522249   \n",
       "\n",
       "       3rd_stop_wait_minutes        stops  ticket_price_usd  \\\n",
       "count              17.000000  2459.000000       2455.000000   \n",
       "mean              356.764706     1.294429       1316.544603   \n",
       "min               176.000000     0.000000        135.000000   \n",
       "25%               176.000000     1.000000        771.000000   \n",
       "50%               200.000000     1.000000       1128.000000   \n",
       "75%               346.000000     2.000000       1607.000000   \n",
       "max              1385.000000     3.000000       7867.000000   \n",
       "std               391.952569     0.517722        885.885657   \n",
       "\n",
       "                    arrival_datetime  \n",
       "count                           2295  \n",
       "mean   2025-04-08 19:46:51.320261376  \n",
       "min              2025-04-08 00:20:00  \n",
       "25%              2025-04-08 11:30:00  \n",
       "50%              2025-04-08 18:03:00  \n",
       "75%              2025-04-09 00:05:00  \n",
       "max              2025-04-10 11:00:00  \n",
       "std                              NaN  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335072d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Summarize the dataset using info() and describe()\n",
    "print(\"DataFrame Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nDataFrame Description:\")\n",
    "print(df.describe(include='all'))\n",
    "\n",
    "# Check for duplicate rows\n",
    "duplicate_rows = df.duplicated().sum()\n",
    "print(f\"\\nNumber of duplicate rows: {duplicate_rows}\")\n",
    "\n",
    "# Check for duplicate values in a given column (example: 'tailnum')\n",
    "duplicate_values_tailnum = df['tailnum'].duplicated().sum()\n",
    "print(f\"\\nNumber of duplicate values in 'tailnum': {duplicate_values_tailnum}\")\n",
    "\n",
    "# Calculate mean, median, and mode of each column\n",
    "# mean_values = df.mean()\n",
    "# median_values = df.median()\n",
    "# mode_values = df.mode().iloc[0]\n",
    "\n",
    "# print(\"\\nMean values:\")\n",
    "# print(mean_values)\n",
    "# print(\"\\nMedian values:\")\n",
    "# print(median_values)\n",
    "# print(\"\\nMode values:\")\n",
    "# print(mode_values)\n",
    "\n",
    "# Check for missing or null values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"\\nMissing values:\")\n",
    "print(missing_values)\n",
    "\n",
    "# # Fill missing values with mean (example: 'temperature')\n",
    "# df['temperature'].fillna(df['temperature'].mean(), inplace=True)\n",
    "# df['windspeed'].fillna(df['windspeed'].mean(), inplace=True)\n",
    "# df['windspeedsquare'].fillna(df['windspeedsquare'].mean(), inplace=True)\n",
    "# df['windgustspeed'].fillna(df['windgustspeed'].mean(), inplace=True)\n",
    "\n",
    "# Identify inconsistent data (example: 'year' should be reasonable)\n",
    "inconsistent_data = df[df['year'] < 1900]\n",
    "print(\"\\nInconsistent data (year < 1900):\")\n",
    "print(inconsistent_data)\n",
    "\n",
    "# Encode categorical variables (example: 'origin', 'dest', 'uniquecarrier')\n",
    "df_encoded = pd.get_dummies(df, columns=['origin', 'dest', 'uniquecarrier', 'origincityname', 'originstate'])\n",
    "\n",
    "print(\"\\nDataFrame with encoded categorical variables:\")\n",
    "print(df_encoded.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abab9e6d-18cc-4863-b980-3e52f581763a",
   "metadata": {},
   "source": [
    "# 4. Storytelling With Data graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1911148d-9df6-4b33-a875-8c96408ec834",
   "metadata": {},
   "source": [
    "Just like last week: choose any graph in the Introduction of Storytelling With Data. Use matplotlib to reproduce it in a rough way. I don't expect you to spend an enormous amount of time on this; I understand that you likely will not have time to re-create every feature of the graph. However, if you're excited about learning to use matplotlib, this is a good way to do that. You don't have to duplicate the exact values on the graph; just the same rough shape will be enough.  If you don't feel comfortable using matplotlib yet, do the best you can and write down what you tried or what Google searches you did to find the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2888f9-3700-45ab-9829-6a5372106f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
